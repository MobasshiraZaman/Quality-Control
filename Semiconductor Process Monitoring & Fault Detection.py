# -*- coding: utf-8 -*-
"""AQC Short Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJQOmqZKQ2nXATNPHQTeolxOdKMrQDjC
"""

# AQC 670
# Mobasshira Zaman
# Nov 2023

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# for SMOTE
!pip install -q imbalanced-learn
from imblearn.over_sampling import SMOTE

df = pd.read_csv("secom.data", sep=r"\s+", header=None)
labels = pd.read_csv("secom_labels.data", sep=r"\s+", header=None,
                     names=["label_raw", "timestamp"])

print(df.shape, labels.shape)


y = (labels["label_raw"] == 1).astype(int)
df["label"] = y

feature_cols = [c for c in df.columns if c != "label"]
print("Features:", len(feature_cols), "  Faulty samples:", y.sum())

# One sensor feature for SPC (first column)
feature_name = feature_cols[0]
x = df[feature_name].copy()
x = x.dropna().reset_index(drop=True)
# Subgrouping: n = 5
n = 5
k = len(x) // n
x_trim = x.iloc[:k * n].values
groups = x_trim.reshape(k, n)

xbar = groups.mean(axis=1)
R = np.ptp(groups, axis=1)

subgroups = pd.DataFrame({
    "subgroup": np.arange(k),
    "xbar": xbar,
    "R": R
})

# Xbar-R constants (n = 5)
A2 = 0.577
D3 = 0.0
D4 = 2.114

xbar_bar = subgroups["xbar"].mean()
R_bar = subgroups["R"].mean()

ucl_x = xbar_bar + A2 * R_bar
lcl_x = xbar_bar - A2 * R_bar
ucl_R = D4 * R_bar
lcl_R = D3 * R_bar

subgroups["ooc_xbar"] = (subgroups["xbar"] > ucl_x) | (subgroups["xbar"] < lcl_x)
subgroups["ooc_R"]   = (subgroups["R"]   > ucl_R)  | (subgroups["R"]   < lcl_R)

# --- X̄ chart ---
plt.figure(figsize=(8, 4))
plt.plot(subgroups["subgroup"], subgroups["xbar"], marker="o")
plt.axhline(xbar_bar, color="black", linestyle="--", label="Center")
plt.axhline(ucl_x, color="red", linestyle="--", label="UCL")
plt.axhline(lcl_x, color="red", linestyle="--", label="LCL")

ooc = subgroups[subgroups["ooc_xbar"]]
if not ooc.empty:
    plt.scatter(ooc["subgroup"], ooc["xbar"], color="red", marker="x", s=60)

plt.title(f"X̄ Chart – Feature {feature_name}")
plt.xlabel("Subgroup")
plt.ylabel("Mean")
plt.legend()
plt.tight_layout()
plt.show()

# --- R chart ---
plt.figure(figsize=(8, 4))
plt.plot(subgroups["subgroup"], subgroups["R"], marker="o")
plt.axhline(R_bar, color="black", linestyle="--", label="Center")
plt.axhline(ucl_R, color="red", linestyle="--", label="UCL")
plt.axhline(lcl_R, color="red", linestyle="--", label="LCL")

ooc_R = subgroups[subgroups["ooc_R"]]
if not ooc_R.empty:
    plt.scatter(ooc_R["subgroup"], ooc_R["R"], color="red", marker="x", s=60)

plt.title(f"R Chart – Feature {feature_name}")
plt.xlabel("Subgroup")
plt.ylabel("Range")
plt.legend()
plt.tight_layout()
plt.show()

X = df[feature_cols]

imputer = SimpleImputer(strategy="mean")
X_imp = imputer.fit_transform(X)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, stratify=y, random_state=42
)

iso = IsolationForest(
    n_estimators=200,
    contamination=0.1,
    random_state=42
)
iso.fit(X_train)

scores = iso.decision_function(X_test)
labels_pred = iso.predict(X_test)   # 1 = normal, -1 = anomaly

# Map to 0/1 for comparison: anomaly -> 1, normal -> 0
y_pred = np.where(labels_pred == -1, 1, 0)

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(y_test, y_pred))
print("\nClassification report:")
print(classification_report(y_test, y_pred, digits=4))

plt.figure(figsize=(8, 4))
plt.scatter(np.arange(len(scores)), scores, s=8)
plt.title("Isolation Forest anomaly scores (test set)")
plt.xlabel("Sample index")
plt.ylabel("Score (higher = more normal)")
plt.tight_layout()
plt.show()

# Redo split from the scaled data, just to be explicit
from sklearn.model_selection import train_test_split

X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
    X_scaled, y, test_size=0.3, stratify=y, random_state=42
)

# PCA: keep 95% of variance (you can change to n_components=20 if you want fixed size)
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_full)
X_test_pca = pca.transform(X_test_full)

print("Original dimension:", X_scaled.shape[1])
print("Reduced dimension:", X_train_pca.shape[1])

# Apply SMOTE on PCA-reduced training data
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_pca, y_train_full)

print("Class distribution before SMOTE:", dict(zip(*np.unique(y_train_full, return_counts=True))))
print("Class distribution after SMOTE:", dict(zip(*np.unique(y_train_res, return_counts=True))))

log_clf = LogisticRegression(
    max_iter=2000,
    class_weight=None,  # SMOTE already balanced it
    random_state=42
)
log_clf.fit(X_train_res, y_train_res)

y_pred_log = log_clf.predict(X_test_pca)

print("=== Logistic Regression on PCA + SMOTE ===")
print("Confusion matrix:")
print(confusion_matrix(y_test_full, y_pred_log))
print("\nClassification report:")
print(classification_report(y_test_full, y_pred_log, digits=4))

rf_clf = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)
rf_clf.fit(X_train_res, y_train_res)

y_pred_rf = rf_clf.predict(X_test_pca)

print("=== Random Forest on PCA + SMOTE ===")
print("Confusion matrix:")
print(confusion_matrix(y_test_full, y_pred_rf))
print("\nClassification report:")
print(classification_report(y_test_full, y_pred_rf, digits=4))

# Recreate x if needed
feature_name = feature_cols[0]
x = df[feature_name].copy()
x = x.dropna().reset_index(drop=True)

import numpy as np
import matplotlib.pyplot as plt

lam = 0.2  # smoothing parameter
L = 3.0    # control limit width (like 3-sigma)

mu = x.mean()
sigma = x.std(ddof=1)

# EWMA statistic
z = []
prev = x.iloc[0]
for val in x:
    current = lam * val + (1 - lam) * prev
    z.append(current)
    prev = current
z = pd.Series(z, index=x.index)

# Time-varying control limits
t = np.arange(1, len(x) + 1)
factor = np.sqrt((lam / (2 - lam)) * (1 - (1 - lam) ** (2 * t)))
ucl = mu + L * sigma * factor
lcl = mu - L * sigma * factor

plt.figure(figsize=(8, 4))
plt.plot(z.index, z.values, label="EWMA", marker="o", markersize=2, linewidth=1)
plt.axhline(mu, color="black", linestyle="--", label="Center")
plt.plot(x.index, ucl, linestyle="--", label="UCL")
plt.plot(x.index, lcl, linestyle="--", label="LCL")

plt.title("EWMA Chart – Feature 0")
plt.xlabel("Time index")
plt.ylabel("EWMA statistic")
plt.legend()
plt.tight_layout()
plt.show()

mu = x.mean()
sigma = x.std(ddof=1)

k = 0.5 * sigma   # reference value (often 0.5 * sigma)
h = 5 * sigma     # decision interval / control limit

C_pos = [0.0]
C_neg = [0.0]

for i in range(1, len(x)):
    s_pos = max(0, C_pos[-1] + (x.iloc[i] - mu - k))
    s_neg = max(0, C_neg[-1] - (x.iloc[i] - mu + k))
    C_pos.append(s_pos)
    C_neg.append(s_neg)

C_pos = pd.Series(C_pos, index=x.index)
C_neg = pd.Series(C_neg, index=x.index)

plt.figure(figsize=(8, 4))
plt.plot(C_pos.index, C_pos.values, label="CUSUM +", linewidth=1)
plt.plot(C_neg.index, C_neg.values, label="CUSUM -", linewidth=1)
plt.axhline(h, color="red", linestyle="--", label="Decision limit")
plt.title("Two-sided CUSUM Chart – Feature 0")
plt.xlabel("Time index")
plt.ylabel("CUSUM statistic")
plt.legend()
plt.tight_layout()
plt.show()

iso_pca = IsolationForest(
    n_estimators=200,
    contamination=0.1,
    random_state=42
)
iso_pca.fit(X_train_pca)

scores_pca = iso_pca.decision_function(X_test_pca)
labels_pca = iso_pca.predict(X_test_pca)  # 1 = normal, -1 = anomaly
y_pred_if_pca = np.where(labels_pca == -1, 1, 0)

print("=== Isolation Forest on PCA features ===")
print("Confusion matrix:")
print(confusion_matrix(y_test_full, y_pred_if_pca))
print("\nClassification report:")
print(classification_report(y_test_full, y_pred_if_pca, digits=4))

plt.figure(figsize=(8, 4))
plt.scatter(np.arange(len(scores_pca)), scores_pca, s=8)
plt.title("Isolation Forest Anomaly Scores (PCA space)")
plt.xlabel("Sample index")
plt.ylabel("Score (higher = more normal)")
plt.tight_layout()
plt.show()